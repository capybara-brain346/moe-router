# moe-router
A small Mixture-of-Experts (MoE) Transformer trained from scratch to demonstrate how modern sparse expert models (Mixtral, GLaM, Switch Transformer) achieve more model capacity without increasing per-token compute.
