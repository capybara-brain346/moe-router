# moe-router

A small Mixture-of-Experts (MoE) Transformer trained from scratch to learn how sparse expert models work and to study their performance, routing behavior, and efficiency under realistic conditions.
